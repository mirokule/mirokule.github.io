<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Unity, iOS, Swift, ML" />










<meta name="description" content="万人迷">
<meta property="og:type" content="website">
<meta property="og:title" content="Gate of Babylon">
<meta property="og:url" content="http://mirokule.github.io/page/2/index.html">
<meta property="og:site_name" content="Gate of Babylon">
<meta property="og:description" content="万人迷">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gate of Babylon">
<meta name="twitter:description" content="万人迷">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://mirokule.github.io/page/2/"/>





  <title>Gate of Babylon</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Gate of Babylon</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">不积跬步，无以至千里</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/06/07/RL-ValueIteration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/07/RL-ValueIteration/" itemprop="url">Value Iteration</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-07T14:30:51+08:00">
                2019-06-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  339
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>动态规划算法的一种，用于求解有限空间、模型p(s’,r|s,a)已知的问题。<br>算法基于值函数，利用了动态规划问题的特点，即全局最优解在任何子问题上也是最优解。</p>
<h4 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h4><p>1.没有预先指定的策略policy；<br>2.遍历计算所有的q(s,a)，共 count(state) x count(action) 个；<br>3.每轮迭代中，v(s)和q(s,a)都会更新；和policy iteration相比，不需要等待v(s)收敛；<br>4.q(s,a)计算公式：<br>$$ q_{\pi}(s,a) = \sum_{s’,r} p(s’,r|s,a) [r+\gamma v_{\pi}(s’)] $$</p>
<p>5.v(s)使用Bellman Optimality Equation取最大值，而不是Bellman Expectation：<br>$$ v_{k+1}(s) = \underset{a}{max} \sum_{s’,r}p(s’,r|s,a)[r+\gamma v_{k}(s’)] $$</p>
<p>这一步对action的选择，实际上生成了策略，并给出了新策略v(s)的评估。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">value_iteration</span><span class="params">(env, gamma=<span class="number">1.0</span>, max_iterations=<span class="number">10000</span>, threshold=<span class="number">1e-10</span>)</span>:</span></span><br><span class="line">    value_table = np.zeros(env.observation_space.n)</span><br><span class="line">    policy = np.zeros(env.observation_space.n)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iterations):</span><br><span class="line">        updated_value_table = np.copy(value_table)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> state <span class="keyword">in</span> range(env.observation_space.n):</span><br><span class="line">            Q_values = np.zeros(env.action_space.n)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> range(env.action_space.n):    </span><br><span class="line">                next_rewards = []</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> trans_prob, next_state, reward, _ <span class="keyword">in</span> env.P[state][action]: <span class="comment"># env.P[s][a] -&gt; list</span></span><br><span class="line">                    next_state_reward = trans_prob * (reward + gamma * updated_value_table[next_state])</span><br><span class="line">                    next_rewards.append(next_state_reward)</span><br><span class="line">                    </span><br><span class="line">                Q_values[action] = sum(next_rewards)</span><br><span class="line"></span><br><span class="line">            value_table[state] = np.max(Q_values)</span><br><span class="line">            policy[state] = np.argmax(Q_values)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> np.sum(np.fabs(updated_value_table - value_table)) &lt;= threshold:</span><br><span class="line">            print(<span class="string">"Value iteration converged at iteration %d."</span> %(i+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> policy, value_table</span><br><span class="line"></span><br><span class="line">optimal_policy, optimal_values = value_iteration(env=env, gamma=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/27/TF-Reduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/27/TF-Reduce/" itemprop="url">Reduce_XXX</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-27T14:29:45+08:00">
                2019-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  150
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>TensorFlow中有一类运算函数具有相同的参数，比如reduce_mean, reduce_min, reduce_max, reduce_sum, reduce_std等。可以对输入tensor按指定轴运算。<br>以reduce_mean为例，可以写作tf.reduce_mean或者tf.math.reduce_mean:</p>
<p>tf.math.reduce_mean(<br>    input_tensor,<br>    axis = None,<br>    keepdims = False,<br>    name = None<br>)</p>
<ul>
<li>input_tensor: 输入</li>
<li>axis: 执行运算的轴，如不指定则为全部数据</li>
<li>keepdims: 运算结果是否保持维度，默认否，降为1维</li>
<li>name: 可选名称</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1.</span>, <span class="number">1.</span>], [<span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line">tf.reduce_mean(x) <span class="comment"># 1.5 不指定axis，则对全部数据取平均</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">0</span>) <span class="comment"># [1.5, 1.5] 纵轴计算</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">1</span>) <span class="comment"># [1., 2.] 横轴计算</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/27/TF-NeuralNetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/27/TF-NeuralNetwork/" itemprop="url">Neural Network by TensorFlow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-27T11:38:34+08:00">
                2019-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  224
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>TensorFlow实例：简单神经网络<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learing_rate = <span class="number">0.01</span></span><br><span class="line">num_steps = <span class="number">500</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">n_hidden_1 = <span class="number">256</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span></span><br><span class="line">num_input = <span class="number">784</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, num_input])</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line">weights = &#123;</span><br><span class="line">	<span class="string">'h1'</span>: tf.Variable(tf.random_normal([num_input, n_hidden_1])),</span><br><span class="line">	<span class="string">'h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">	<span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_2, num_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">	<span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">	<span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">	<span class="string">'out'</span>: tf.Variable(tf.random_normal([num_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neural_net</span><span class="params">(x)</span>:</span></span><br><span class="line">	layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'h1'</span>]), biases[<span class="string">'b1'</span>])</span><br><span class="line">	layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'h2'</span>]), biases[<span class="string">'b2'</span>])</span><br><span class="line">	out_layer = tf.matmul(layer_2, weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</span><br><span class="line">	<span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line">pred = neural_net(X)</span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">correct_pred = tf.equal(tf.argmax(pred,<span class="number">1</span>), tf.argmax(Y,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line">		batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">		sess.run(optimizer, feed_dict=&#123;X: batch_x, Y: batch_y&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> step % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">1</span>:</span><br><span class="line">			loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X:batch_x, Y:batch_y&#125;)</span><br><span class="line">			print(<span class="string">"Traing Accuracy="</span> + <span class="string">"&#123;:.3f&#125;"</span>.format(acc))</span><br><span class="line">	</span><br><span class="line">	print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">	print(<span class="string">"Testing Accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;X:mnist.test.images, Y:mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/27/TF-LogisticRegression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/27/TF-LogisticRegression/" itemprop="url">Logistic Regression by TensorFlow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-27T11:27:09+08:00">
                2019-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  136
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Tensoflow实例：Logistic Regression</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">pred = tf.nn.sigmoid(tf.matmul(x, W) + b)</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reducetion_indices=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y)</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">		avg_cost = <span class="number">0</span></span><br><span class="line">		total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">			batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">			_, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">			avg_cost += c / total_batch</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">			<span class="keyword">print</span> <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">print</span> <span class="string">"optimization finished!"</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/27/TF-LinearRegression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/27/TF-LinearRegression/" itemprop="url">Linear Regression by TensorFlow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-27T10:40:32+08:00">
                2019-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  136
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>TensorFlow实例：线性回归<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">1000</span></span><br><span class="line">display_step = <span class="number">50</span></span><br><span class="line">n_samples = train_X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">Y = tf.plackholder(<span class="string">"float"</span>)</span><br><span class="line"></span><br><span class="line">W = tf.Variable(rng.randn(), name=<span class="string">"weight"</span>)</span><br><span class="line">b = tf.Variable(rng.randn(), name=<span class="string">"bias"</span>)</span><br><span class="line"></span><br><span class="line">pred = tf.add(tf.multiply(X,W), b)</span><br><span class="line">cost = tf.reduce_sum(tf.pow(pred-Y, <span class="number">2</span>)) / (<span class="number">2</span>*n_samples)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Fit training data</span></span><br><span class="line">	<span class="keyword">for</span> epoch <span class="keyword">in</span> range(traing_epochs):</span><br><span class="line">		<span class="keyword">for</span> (x,y) <span class="keyword">in</span> zip(train_X, train_Y):</span><br><span class="line">			sess.run(optimizer, feed_dict=&#123;X:x, Y:y&#125;)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">			c = sess.run(cost, feed_dict=&#123;X:train_X, Y:train_Y)</span><br><span class="line">			print(<span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(c), <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b))</span><br><span class="line">	</span><br><span class="line">	print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">	traing_cost = sess.run(cost, feed_dict=&#123;X:train_X, Y:train_Y&#125;)</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/27/Python-NumPy-Stack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/27/Python-NumPy-Stack/" itemprop="url">Stack, vstack, hstack, dstack</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-27T10:00:16+08:00">
                2019-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  243
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="1-stack"><a href="#1-stack" class="headerlink" title="1.stack()"></a>1.stack()</h4><p>numpy.stack(arrays, axis=0, out=None)<br>沿一条<strong>新轴</strong>(new axis)将array堆叠，axis=0代表第一条轴，axis=-1代表最后一条。<br>会改变原array形状。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">np.stack((a,b))</span><br><span class="line"><span class="comment"># array([[1,2,3], [4,5,6]])</span></span><br><span class="line"></span><br><span class="line">np.stack((a,b), axis=<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># array([[1,4], [2,5], [3,6]])</span></span><br></pre></td></tr></table></figure></p>
<h4 id="2-vstack"><a href="#2-vstack" class="headerlink" title="2.vstack()"></a>2.vstack()</h4><p>numpy.vstack(tup)，将array沿第一轴(纵向)堆叠在一起，要求array在其他轴上形状相同。<br>等价于np.concatenate(tup, axis=0)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">np.vstack((a,b))</span><br><span class="line"><span class="comment"># array([[1,2,3], [2,3,4]])</span></span><br></pre></td></tr></table></figure></p>
<h4 id="3-hstack"><a href="#3-hstack" class="headerlink" title="3.hstack()"></a>3.hstack()</h4><p>numpy.hstack(tup)，将array沿第二轴(横向)堆叠在一起，要求array在其他轴上形状相同。<br>等价于np.concatenate(tup, axis=1)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">b = np.array((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">np.hstack((a,b))</span><br><span class="line"><span class="comment"># array([1,2,3,2,3,4])</span></span><br></pre></td></tr></table></figure></p>
<h4 id="4-dstack"><a href="#4-dstack" class="headerlink" title="4.dstack()"></a>4.dstack()</h4><p>numpy.dstack(tup)，沿第三轴堆叠。<br>等价于np.concatenate(tup, axis = 2)</p>
<h4 id="5-concatenate"><a href="#5-concatenate" class="headerlink" title="5.concatenate()"></a>5.concatenate()</h4><p>沿一条<strong>现有的轴</strong>(existing axis)将array堆叠，不会改变原array形状</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/15/Unity-Destroy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/15/Unity-Destroy/" itemprop="url">Destroy, Enable, SetActive</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-15T15:04:37+08:00">
                2019-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  157
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Unity中让GameObject消失的方法有3种：</p>
<ul>
<li>Destroy</li>
<li>Enable</li>
<li>SetActive(false)</li>
</ul>
<h4 id="1-Destroy"><a href="#1-Destroy" class="headerlink" title="1.Destroy"></a>1.Destroy</h4><p>标记销毁。<br><strong>该物体不会立即释放</strong>，而是在下下个场景中释放内存资源，比如A场景标记后在C场景中释放。</p>
<h4 id="2-Enable"><a href="#2-Enable" class="headerlink" title="2.Enable"></a>2.Enable</h4><p>标记显示。<br>物体仍然存在，但不会在屏幕上显示，相当于隐身。</p>
<h4 id="3-SetActive"><a href="#3-SetActive" class="headerlink" title="3.SetActive"></a>3.SetActive</h4><p>标记停用。<br>会将所有组件启用或停用。如果有脚本，会执行OnEnable()或OnDisable()方法。</p>
<h4 id="4-移动到屏幕外"><a href="#4-移动到屏幕外" class="headerlink" title="4.移动到屏幕外"></a>4.移动到屏幕外</h4><p>将物体移动到屏幕外以隐藏，移动到屏幕内以显示。<br>可以降低频繁切换的计算资源消耗。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/09/ML-LightGBM-Parameters/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/ML-LightGBM-Parameters/" itemprop="url">LightGBM 参数调整</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-09T16:11:57+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  231
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>同其他决策树模型的调参步骤类似：</p>
<ol>
<li>提高学习率，加快训练速度</li>
<li>调整树模型基本参数</li>
<li>调整正则化参数</li>
<li>降低学习率，训练模型并验证</li>
</ol>
<h5 id="树模型参数"><a href="#树模型参数" class="headerlink" title="树模型参数"></a>树模型参数</h5><p>1.boost/boosting/boosting_type: 估计器类型，一般使用默认的gbdt<br>2.n_estimators/num_iterations/num_round/num_boost_round: 最大迭代次数，可以设为大数，然后使用early_stopping<br>3.max_depth: 树的深度越大越容易过拟合，可设为-1<br>4.<strong>num_leaves</strong>: <strong>最重要的参数</strong>，由于LightGBM使用leaf_wise算法，该参数直接决定深度<br>5.min_data_in_leaf/ min_child_samples: 取决于训练样本个数和num_leaves<br>6.min_sum_hessian_in_leaf/ min_child_weight: 使一个节点分裂的最小海森值之和 </p>
<h5 id="正则化参数"><a href="#正则化参数" class="headerlink" title="正则化参数"></a>正则化参数</h5><p>1.feature_fraction: 每次建树使用的特征比例<br>2.bagging_fraction: 每次bagging的比例<br>3.bagging_freq: 默认为0，不使用bagging; 设置为k意味着每k轮迭代进行一次bagging<br>4.lambda_l1<br>5.lambda_l2  </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/04/03/Unity-UpdateOrder/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/03/Unity-UpdateOrder/" itemprop="url">Upate, FixUpdate, LateUpdate</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-03T10:31:12+08:00">
                2019-04-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  128
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="FixedUpdate"><a href="#FixedUpdate" class="headerlink" title="FixedUpdate"></a>FixedUpdate</h5><p>固定时间间隔更新，与帧率无关。<br>如果帧率很低，可能一帧内更新多次；如果帧率很高，可能多帧才更新一次。<br>在FixedUpdate内计算运动轨迹时不需要乘以时间间隔Time.deltaTime.</p>
<h5 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h5><p>每帧执行一次，更新的主力函数。</p>
<h5 id="LateUpdate"><a href="#LateUpdate" class="headerlink" title="LateUpdate"></a>LateUpdate</h5><p>每帧执行一次，在Update函数完成之后。<br>常见应用包括第三人称摄像机的位置和转矩更新，以确保Update中角色的移动已经完成。</p>
<p><img src="\img\updateOrder.svg"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/01/18/Python-pandas-Toolbox/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/18/Python-pandas-Toolbox/" itemprop="url">Pandas小抄</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-18T16:03:19+08:00">
                2019-01-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  76
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="读取Json"><a href="#读取Json" class="headerlink" title="读取Json"></a>读取Json</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_df</span><span class="params">(csv_path=<span class="string">'../input/train.csv'</span>, nrows=None)</span>:</span></span><br><span class="line">    JSON_COLUMNS = [<span class="string">'device'</span>, <span class="string">'geoNetwork'</span>, <span class="string">'totals'</span>, <span class="string">'trafficSource'</span>]</span><br><span class="line">    </span><br><span class="line">    df = pd.read_csv(csv_path, </span><br><span class="line">                     converters=&#123;column: json.loads <span class="keyword">for</span> column <span class="keyword">in</span> JSON_COLUMNS&#125;, </span><br><span class="line">                     dtype=&#123;<span class="string">'fullVisitorId'</span>: <span class="string">'str'</span>&#125;, <span class="comment"># Important!!</span></span><br><span class="line">                     nrows=nrows)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> column <span class="keyword">in</span> JSON_COLUMNS:</span><br><span class="line">        column_as_df = json_normalize(df[column])</span><br><span class="line">        column_as_df.columns = [<span class="string">f"<span class="subst">&#123;column&#125;</span>.<span class="subst">&#123;subcolumn&#125;</span>"</span> <span class="keyword">for</span> subcolumn <span class="keyword">in</span> column_as_df.columns]</span><br><span class="line">        df = df.drop(column, axis=<span class="number">1</span>).merge(column_as_df, right_index=<span class="keyword">True</span>, left_index=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f"Loaded <span class="subst">&#123;os.path.basename(csv_path)&#125;</span>. Shape: <span class="subst">&#123;df.shape&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/01/14/RL-AlgorithmStructure/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/14/RL-AlgorithmStructure/" itemprop="url">增强学习算法的一般步骤</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-14T15:12:48+08:00">
                2019-01-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  369
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>没有一种算法可以适应所有的问题。有些问题里表示一个策略是比较容易的，而有些问题去拟合模型更容易。<br>虽然算法有差别，但是几乎所有的增强学习算法都有着同样的架构：</p>
<p><img src="/img/RLStructure.png"></p>
<p>1.生成样本<br>样本效率是最主要的关注点。使用真实物理环境的驾驶、机器人控制问题，和使用仿真环境的模拟器，收集样本的速度会有很大差异。</p>
<ul>
<li>on-policy: 策略每次更改后都要重新生成样本，即训练数据必须由当前策略产生</li>
<li>off-policy: 训练数据可以由其他策略产生，在不重新采样的情况下改变当前策略</li>
</ul>
<p>2.策略评估/模型拟合<br>策略梯度法计算回报的总和 $\sum\,r$，简单;<br>值函数方法拟合 Q(s,a)，复杂;<br>基于模型的方法则需要估计状态转移概率 p(s’|s,a)。</p>
<p>3.策略改进<br>策略梯度法对神经网络做梯度步的调整 $\theta\leftarrow\theta+\alpha\nabla J(\theta)$，复杂；<br>值函数方法on-policy通常采用 $\epsilon-greedy$，off-policy比如Q-Learning直接取最大值 argmaxQ(s,a)，简单;<br>基于模型的方法则需要用梯度方法去优化策略函数。</p>
<table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">评估拟合</th>
<th style="text-align:center">策略改进</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">policy-based</td>
<td style="text-align:center">$\sum\,r$</td>
<td style="text-align:center">$\theta\leftarrow\theta+\alpha\nabla J(\theta)$</td>
</tr>
<tr>
<td style="text-align:center">value-based</td>
<td style="text-align:center">Q(s,a)</td>
<td style="text-align:center">argmaxQ(s,a)</td>
</tr>
<tr>
<td style="text-align:center">model-based</td>
<td style="text-align:center">p(s’/s,a)</td>
<td style="text-align:center">gradient</td>
</tr>
</tbody>
</table>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2019/01/04/RL-PolicyGradient/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/04/RL-PolicyGradient/" itemprop="url">策略梯度方法Policy Gradient</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-04T14:37:42+08:00">
                2019-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1,093
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>当状态空间或动作空间的维度很高，或者连续时，不容易估算值函数$\hat{Q}(s,a)$。此时可以使用基于策略(policy-based)的搜索方法。<br>策略搜索是将策略参数化，使用线性或非线性(神经网络)函数来表示策略，寻找最优的参数$\theta$来使得回报的期望最大。</p>
<p>优点：</p>
<ul>
<li>收敛性更好</li>
<li>高维或连续动作空间里更有效</li>
<li>可以学到随机策略</li>
</ul>
<p>缺点</p>
<ul>
<li>容易收敛到局部最优解</li>
<li>评估策略时通常低效且高方差</li>
</ul>
<h4 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h4><p>用$\tau$表示一次实验中的状态-行为序列$s_0,a_0,s_1,a_1,\cdots,s_H,a_H$，<br>用$R(\tau)=\sum_{t=0}^HR(s_t,a_t)$表示轨迹$\tau$的回报，<br>用$P(\tau;\theta)$表示轨迹$\tau$出现的概率，那么目标函数可以表示为：<br>$$ J(\theta) = \sum_{\tau}P(\tau;\theta) R(\tau) $$</p>
<p>参数$\theta$的梯度就可以写成：<br>$$ \nabla_{\theta} J(\theta) = \sum_{\tau} \nabla_{\theta}P(\tau;\theta) R(\tau) \\<br>= \sum_{\tau} P(\tau;\theta) \frac{\nabla_{\theta}P(\tau;\theta) R(\tau)}{P(\tau;\theta)} \\<br>= \sum_{\tau} P(\tau;\theta) \nabla_{\theta}log\,P(\tau;\theta)R(\tau) \\<br>= E[\nabla_{\theta}log\,P(\tau;\theta) R(\tau)]$$</p>
<p>最终策略梯度变成了一个期望，就可以用经验平均来估算。即用当前策略$\pi_{\theta}$采样m条轨迹后，用m条轨迹的平均来逼近策略梯度：<br>$$ \nabla_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m \nabla_{\theta} log\,P(\tau;\theta) R(\tau) $$</p>
<p>从直观上理解，由于$R(\tau)$的存在，策略梯度会增加高回报路径的概率，减少低回报路径的概率。<br>其中，由于$P(\tau,\theta)=\prod_{t=0}^H P(s_{t+1}|s_t,a_t) \pi(a_t|s_t)$，前半部分转移概率P(s;s,a)不含有参数$\theta$可以在求导过程中消掉，所以<br>$$ \nabla_{\theta}log\,P(\tau;\theta) = \sum_{t=0}^H \nabla_{\theta}log\,\pi_{\theta}(a_t|s_t) $$</p>
<p>于是我们得到一个简单的策略梯度法REINFORCE:</p>
<ol>
<li>运行策略$\pi_{\theta}(a|s)，抽取样本$</li>
<li>估计梯度 $\nabla_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m {\sum_{t=0}^H \nabla_{\theta}log\,\pi_{\theta}(a_t|s_t) R(\tau)}$</li>
<li>根据梯度更新参数 $\theta = \theta + \alpha \nabla_{\theta} J(\theta)$</li>
</ol>
<p>形式简单，同样效果也不太好，需要做一些改进。</p>
<h6 id="改进一-基准线baseline"><a href="#改进一-基准线baseline" class="headerlink" title="改进一: 基准线baseline"></a>改进一: 基准线baseline</h6><p>奖励函数$R(\tau)$的大小会通过梯度影响参数移动的步伐。我们希望不同轨迹的奖励函数有正有负，这样参数移动会增加好轨迹的概率，降低坏轨迹的概率，更快收敛。因而引入基准值b，将所有的奖励函数都减去基准值。<br>$$ \nabla_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^H \nabla_{\theta}log\,\pi_{\theta}(a_t|s_t) (R(\tau)-b) $$</p>
<p>通常使用 $b = \frac{1}{N} \sum_{i=1}^{N} r(\tau_i)$，并不是理论最优解，但形式简单，实际效果也和最优解差不多。</p>
<h6 id="改进二-因果关系casuality"><a href="#改进二-因果关系casuality" class="headerlink" title="改进二: 因果关系casuality"></a>改进二: 因果关系casuality</h6><p>对每一条轨迹，梯度都乘以了固定的乘数(R-b)，这实际上是不公平的，比如后发生的动作应该与之前的奖励无关。因此把轨迹的回报(R-b)细分到每个动作的回报($\sum r-b$)，对每个动作$a_t$，只计算t时刻之后的回报(reward to go):<br>$$ \nabla_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{H-1} \nabla_{\theta}log\,\pi_{\theta}(a_t|s_t) (\sum_{k=t}^{H-1}(r(s_k)-b)) $$</p>
<h4 id="Off-Policy"><a href="#Off-Policy" class="headerlink" title="Off-Policy"></a>Off-Policy</h4><p>策略梯度法是一个on-policy算法，求期望时必须从当前分布上采样才具有无偏性。使用采集到的轨迹数据计算梯度并更新参数$\theta$后，策略$\pi_{\theta}$随之改变。重新采集数据需要耗费大量的资源，因而在off-policy策略里，使用两套参数：固定$\theta’$，用$\pi_{\theta’}$采集的数据来更新$\theta$.<br>两套策略对应的数据分布不同，就涉及到重要性采样：<br>$$ E_{x\sim p(x)}f(x) = \int p(x)f(x) \\<br>= \int q(x) \frac{p(x)}{q(x)}f(x) \\<br>= E_{x\sim q(x)} \frac{p(x)}{q(x)} f(x) $$</p>
<p>虽然期望相等，但是方差并不相同。使得在采样数据不足时，结果可能有较大波动。<br>On-Policy:<br>$$ \nabla_{\theta} J(\theta) = E_{\tau\sim p(\tau;\theta)}[\nabla_{\theta}log\,P(\tau;\theta) R(\tau)]$$<br>Off-Policy:<br>$$ \nabla_{\theta} J(\theta) = E_{\tau\sim p(\tau;\theta’)}[\frac{P(\tau;\theta)}{P(\tau;\theta’)}\nabla_{\theta}log\,P(\tau;\theta) R(\tau)]$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/12/28/RL-DQN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/28/RL-DQN/" itemprop="url">Deep Q-Learning Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-28T11:24:33+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  910
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>名称辨析：</p>
<ul>
<li>Q-Learning: 时间差分TD的异策略(off-policy control)方法；</li>
<li>Network: 值函数逼近的非线性内核；</li>
<li>Deep: 深度学习。</li>
</ul>
<p>DQN对Q-Learning的改进主要体现在3个方面：</p>
<ul>
<li>使用深度卷积神经网络：处理输入的游戏屏幕图像</li>
<li>训练过程使用经验回放(受海马体启发)：打破数据关联</li>
<li>用目标网络单独处理TD偏差</li>
</ul>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="/img/DQN.png"></p>
<p>1.卷积层没有使用池化(pooling)。<br>池化层的作用更多是目标检测，即检测图片中是否包含特定物体，而不关心物体的位置；<br>与之相反的是，玩游戏时物体的位置非常重要，而图像中包含的物体数量几乎有限而确定，所以没有添加池化层。</p>
<p>2.输出层一次给出所有的Q(s,a)<br>如果将(s,a)作为网络的输入来计算Q值，那么对每个a，都要从头到尾计算一次。这对于CNN来说计算量太大。<br>故只使用屏幕图像(s)作为输入，输出层对应count(a)个节点，一次计算给出所有的Q(s,a)值。</p>
<p>3.输入层包括前4帧画面<br>为了得到物体的运动轨迹，判断当前静态画面中物体的速度方向，将前4帧的历史画面一并作为输入。</p>
<h4 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h4><p>训练网络的数据格式为&lt;s,a,r,s’&gt;，是智能体在实验中得到的经验。<br>在同一个episode里，实验数据前后关联，如果依次用来作为训练数据输入网络，会使得网络过拟合，学习到特定episode的知识。而下个episode的数据到来时，网络面对全新的数据又会不稳定。<br>故将每次的实验数据保留到一个集合中，训练时从数据集里抽样，由来自不同episode的数据来打破关联性。<br>保留数据集的容量有上限。到达上限同时又有新数据进入时，将删除旧数据。</p>
<h4 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h4><p>损失函数仍然使用目标值和预测值的偏差平方：<br>$$ Loss = (r+\gamma\,\underset{a’}{max}\,Q(s’,a’;\theta) - Q(s,a;\theta))^2$$</p>
<p>由于目标值里也包含参数$\theta$，而且这个$\theta$不在后向传播时同步更新，因而目标值和预测值的差距会越来越大。因而引入两个网络，每隔一段时间将训练网络的参数拷贝到目标网络中。这时目标网络中的$\theta$可以看做另一个参数，更新方式为：<br>$$ \theta_{t+1} = \theta_t + \alpha[r + \gamma\,\underset{a’}{max}\,Q(s’,a’;\theta^-)]\nabla Q(s,a;\theta)$$</p>
<h4 id="部分实现"><a href="#部分实现" class="headerlink" title="部分实现"></a>部分实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">        done = <span class="keyword">False</span></span><br><span class="line">        obs = env.reset()</span><br><span class="line">        epoch = <span class="number">0</span></span><br><span class="line">        episodic_reward = <span class="number">0</span></span><br><span class="line">        actions_counter = Counter() </span><br><span class="line">        episodic_loss = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            obs = preprocess_observation(obs)</span><br><span class="line"></span><br><span class="line">	    <span class="comment"># 根据当前屏幕状态s1评估所有的Q(s1,a)</span></span><br><span class="line">            actions = mainQ_outputs.eval(feed_dict=&#123;X:[obs], in_training_mode:<span class="keyword">False</span>&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 获得动作</span></span><br><span class="line">            action = np.argmax(actions, axis=<span class="number">-1</span>)</span><br><span class="line">            actions_counter[str(action)] += <span class="number">1</span> </span><br><span class="line">            action = epsilon_greedy(action, global_step)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 执行动作</span></span><br><span class="line">            next_obs, reward, done, _ = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存记录</span></span><br><span class="line">            exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 一段时间后开始训练</span></span><br><span class="line">            <span class="keyword">if</span> global_step % steps_train == <span class="number">0</span> <span class="keyword">and</span> global_step &gt; start_steps:</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 抽样</span></span><br><span class="line">                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)</span><br><span class="line"></span><br><span class="line">                o_obs = [x <span class="keyword">for</span> x <span class="keyword">in</span> o_obs]</span><br><span class="line">                o_next_obs = [x <span class="keyword">for</span> x <span class="keyword">in</span> o_next_obs]</span><br><span class="line">                next_act = mainQ_outputs.eval(feed_dict=&#123;X:o_next_obs, in_training_mode:<span class="keyword">False</span>&#125;)</span><br><span class="line">                y_batch = o_rew + discount_factor * np.max(next_act, axis=<span class="number">-1</span>) * (<span class="number">1</span>-o_done) </span><br><span class="line"></span><br><span class="line">                <span class="comment"># 合并日志 </span></span><br><span class="line">                mrg_summary = merge_summary.eval(feed_dict=&#123;X:o_obs, y:np.expand_dims(y_batch, axis=<span class="number">-1</span>), X_action:o_act, in_training_mode:<span class="keyword">False</span>&#125;)</span><br><span class="line">                file_writer.add_summary(mrg_summary, global_step)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 训练网络</span></span><br><span class="line">                train_loss, _ = sess.run([loss, training_op], feed_dict=&#123;X:o_obs, y:np.expand_dims(y_batch, axis=<span class="number">-1</span>), X_action:o_act, in_training_mode:<span class="keyword">True</span>&#125;)</span><br><span class="line">                episodic_loss.append(train_loss)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 定期拷贝参数</span></span><br><span class="line">            <span class="keyword">if</span> (global_step+<span class="number">1</span>) % copy_steps == <span class="number">0</span> <span class="keyword">and</span> global_step &gt; start_steps:</span><br><span class="line">                copy_target_to_main.run()</span><br><span class="line">                </span><br><span class="line">            obs = next_obs</span><br><span class="line">            epoch += <span class="number">1</span></span><br><span class="line">            global_step += <span class="number">1</span></span><br><span class="line">            episodic_reward += reward</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'Epoch'</span>, epoch, <span class="string">'Reward'</span>, episodic_reward,)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/12/28/RL-Approximation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/28/RL-Approximation/" itemprop="url">值函数逼近</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-28T10:14:34+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  752
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>当状态空间的维数很大，或者为连续空间时，值函数无法用一张表格来表示，不能单独计算出每个值。<br>此时用函数逼近的方法来表示值函数，写作$\hat{v}(s,\theta)$。即把状态s(或者(s,a))作为输入，值函数v或q作为输出。</p>
<p>值函数的内核有不同的选择：</p>
<ul>
<li>线性：线性函数，$\hat{v}=\theta^Ts$</li>
<li>非线性：神经网络</li>
</ul>
<h3 id="1-增量方法-Incremental-Methods"><a href="#1-增量方法-Incremental-Methods" class="headerlink" title="1.增量方法 Incremental Methods"></a>1.增量方法 Incremental Methods</h3><h4 id="1-1-监督学习描述"><a href="#1-1-监督学习描述" class="headerlink" title="1.1 监督学习描述"></a>1.1 监督学习描述</h4><p>用监督学习方法来描述问题，要从数据中学习的是$\theta$，而更新$\theta$会使得所有状态的$\hat{v}(s,\theta)$改变。<br>训练的目标函数为：<br>$$ \underset{\theta}{argmin}(q(s,a)-\hat{q}(s,a,\theta))^2$$</p>
<p>参数的随机梯度更新为：<br>$$ \theta_{t+1} = \theta_t + \alpha[U_t - \hat{v}(S_t,\theta_t)]\nabla\hat{v}(S_t,\theta_t)$$</p>
<p>训练数据可以写作$(S_t,U_t)$，其中$U_t$是：</p>
<ul>
<li>MC: $G_t$</li>
<li>TD(0): $R+\gamma \hat{v}(s’,a’)$</li>
<li>TD($\lambda$): $G_t^{\lambda}$(forward)</li>
</ul>
<p>对于TD(0)方法，目标函数中也包含参数$\theta$，但在求梯度时忽略$\theta$对目标函数$U_t$的影响。这种方法不是完全的梯度法，因此也称为半梯度法。</p>
<h4 id="1-2-基函数"><a href="#1-2-基函数" class="headerlink" title="1.2 基函数"></a>1.2 基函数</h4><p>在线性逼近中，值函数可以写作：<br>$$ \hat{v}(s,\theta) = \theta^T\phi(s) $$</p>
<p>其中$\phi(s)$是状态s的特征函数，也称为基函数。<br>常见的基函数包括：</p>
<ul>
<li>多项式基函数：$s_1, s_2, s_1s_2, s_1^2, s_2^3,\cdots$</li>
<li>傅里叶基函数：$cos(i\pi s)$</li>
<li>径向基函数：$exp(-\frac{\left | s-c_i \right |^2}{2\sigma_i^2})$</li>
</ul>
<h4 id="1-3-收敛性"><a href="#1-3-收敛性" class="headerlink" title="1.3 收敛性"></a>1.3 收敛性</h4><p>训练神经网络时，对数据的假设是<strong>独立同分布</strong>。<br>而强化学习过程采集到的数据是<strong>关联</strong>的，用于训练神经网络并不稳定，因而神经网络往往是不收敛的。</p>
<h3 id="2-批量方法-Batch-Methods"><a href="#2-批量方法-Batch-Methods" class="headerlink" title="2.批量方法 Batch Methods"></a>2.批量方法 Batch Methods</h3><p>增量方法是根据单条数据来更新，虽然计算简单，但样本数据利用率不高。<br>批量更新的方法是将单条数据保留形成数据集，对每个数据集找到最好的拟合函数。<br>打破了增量方法中的数据关联性，因而收敛效果更好。</p>
<h4 id="2-1-最小二乘法"><a href="#2-1-最小二乘法" class="headerlink" title="2.1 最小二乘法"></a>2.1 最小二乘法</h4><p>使用最小二乘法使拟合函数逼近：<br>$$ LS(w) = \sum_{t=1}^T(v_t^{\pi} - \hat{v}(s_t,w))^2 $$<br>此时线性模型的参数更新量为：<br>$$ \Delta w = \alpha [v^{\pi} - \hat{v}(s, w)] \nabla_w \hat{v}(s,w) $$</p>
<p>特别地，线性最小二乘法有矩阵解，时间复杂度$O(N^3)$；作为对比，增量方法的复杂度$O(N^2)$.</p>
<h4 id="2-2-Deep-Q-Networks"><a href="#2-2-Deep-Q-Networks" class="headerlink" title="2.2 Deep Q-Networks"></a>2.2 Deep Q-Networks</h4><p>步骤：<br>(1)根据$\epsilon-greedy$策略采取动作$a_t$<br>(2)将($s_t, a_t, r_{t+1}, s_{t+1}$)存储于数据集D<br>(3)从数据集D中随机抽样一批数据(s,a,r,s’)<br>(4)固定参数$w^-$，计算网络输出Q(s’,a’;$w^-$)<br>(5)目标函数MSE更新网络</p>
<p>$$ L_i(w_i) = E_{s,a,r,s’~D_i} [(r+ \gamma \underset{a’}{max}\,Q(s’,a’;w_i^-) - Q(s,a;w_i))^2] $$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/12/24/RL-MAB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/24/RL-MAB/" itemprop="url">多臂赌博机(MAB)问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-24T11:12:59+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  479
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>多臂赌博机问题研究的是同时具有多条臂来操纵1台老虎机，也可以看做多台单臂老虎机。每条臂的动作会得到不同的回报，需要找出哪条臂会提供最大的回报。<br>每拉动一条臂相当于采取一步行动a，若定义时刻i的回报为$r_i$，则有：<br>$$ Q(a) = \frac{r_1 + r_2 + \cdots + r_k}{k} $$</p>
<p>如果平均实验，即在每条臂上都重复n次，那么绝大多数实验选取的都是非最优解，会使得成本损失较多。<br>如何兼顾exploration和exploit，既能找出回报最大的解，又能在非最优解上减少实验次数？</p>
<h4 id="epsilon-greedy-策略"><a href="#epsilon-greedy-策略" class="headerlink" title="$\epsilon-greedy$策略"></a>$\epsilon-greedy$策略</h4><p>每次以概率$\epsilon$选择当前回报最大的臂，以$1-\epsilon$的概率随机选择一条臂。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilon_greedy</span><span class="params">(epsilon)</span>:</span></span><br><span class="line">    rand = np.random.random()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> rand &lt; epsilon:</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        action = np.argmax(Q)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_rounds):</span><br><span class="line">    arm = epsilon_greedy(<span class="number">0.5</span>)</span><br><span class="line">    </span><br><span class="line">    observation, reward, done, info = env.step(arm)</span><br><span class="line">    </span><br><span class="line">    count[arm] += <span class="number">1</span></span><br><span class="line">    sum_rewards[arm] += reward</span><br><span class="line">    Q[arm] = sum_rewards[arm] / count[arm]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'The optimal arm is &#123;&#125;'</span>.format(np.argmax(Q)))</span><br></pre></td></tr></table></figure></p>
<h4 id="Softmax-Exploration"><a href="#Softmax-Exploration" class="headerlink" title="Softmax Exploration"></a>Softmax Exploration</h4><p>也称作Boltzmann Exploration，是一种产生随机策略的方法。<br>公式如下：<br>$$ P_t(a) = \frac{exp(Q_t(a)/\tau)}{\sum_{i=1}^n exp(Q_t(i)/\tau)}$$</p>
<ul>
<li>引入了Q值和温度参数$\tau$。当$\tau$变大时，每条臂被选择的概率趋于相同；当$\tau$变小时，当前平均回报Q值更大的臂被选择的概率更大。</li>
<li>得到每条臂的概率后，算法会随机一个阈值，当累加概率大于该阈值时则选择当前臂</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(tau)</span>:</span></span><br><span class="line">    total = sum([math.exp(val/tau) <span class="keyword">for</span> val <span class="keyword">in</span> Q])</span><br><span class="line">    probs = [math.exp(val/tau) / total <span class="keyword">for</span> val <span class="keyword">in</span> Q]</span><br><span class="line">    </span><br><span class="line">    threshold = random.random() </span><br><span class="line">    cumulative_prob = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(probs)):</span><br><span class="line">        cumulative_prob += probs[i]</span><br><span class="line">        <span class="keyword">if</span> (cumulative_prob &gt; threshold):</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> np.argmax(probs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_rounds):</span><br><span class="line">    arm = softmax(<span class="number">0.5</span>)</span><br><span class="line">    </span><br><span class="line">    observation, reward, done, info = env.step(arm)</span><br><span class="line">    </span><br><span class="line">    count[arm] += <span class="number">1</span></span><br><span class="line">    sum_rewards[arm] += reward</span><br><span class="line">    Q[arm] = sum_rewwards[arm]/count[arm]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The optimal arm is &#123;&#125;"</span>.format(np.argmax(Q)))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/12/17/ML-Workflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/17/ML-Workflow/" itemprop="url">机器学习的通用工作流程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-17T14:05:24+08:00">
                2018-12-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  825
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本节介绍一种解决机器学习问题的通用模板。</p>
<h3 id="1-定义问题"><a href="#1-定义问题" class="headerlink" title="1.定义问题"></a>1.定义问题</h3><p>明确输入、输出及所使用的数据。注意此阶段有两个假设：</p>
<ul>
<li>假设输出是可以根据输入进行预测的(问题可行)</li>
<li>假设可用数据包含足够多的信息，足以学习输入和输出的关系(数据可行)</li>
</ul>
<p>对于非平稳问题(nonstationary problem，模型随时间变化，比如不同季节的服装销量)，应该用最新数据重新训练模型，或者在一个时间是平稳的尺度上收集数据。</p>
<h3 id="2-选择衡量指标"><a href="#2-选择衡量指标" class="headerlink" title="2.选择衡量指标"></a>2.选择衡量指标</h3><p>对于平衡分类问题，精度和ROC AUC是常用的指标；<br>对于类别不平衡的问题，可以使用准确率和召回率；<br>对于排序问题或多标签分类，可以使用平均准确率均值(mean average precision)；<br>或者其他自定义指标。</p>
<h3 id="3-确定评估方法"><a href="#3-确定评估方法" class="headerlink" title="3.确定评估方法"></a>3.确定评估方法</h3><p>三种常见的评估方法：</p>
<ul>
<li>留出验证法</li>
<li>K-Fold交叉验证</li>
<li>重复的K-Fold交叉验证：用于数据量很少，模型评估又需要非常准确时</li>
</ul>
<h3 id="4-准备数据"><a href="#4-准备数据" class="headerlink" title="4.准备数据"></a>4.准备数据</h3><ul>
<li>将输入数据转换为张量</li>
<li>将张量的取值缩放到较小的区间，比如[-1,1]或者[0,1]</li>
<li>如果不同的特征有不同的取值范围，那么应该做数据标准化</li>
<li>可能需要特征工程</li>
<li>解决重复、缺失等问题</li>
</ul>
<h3 id="5-开发比基准更好的模型"><a href="#5-开发比基准更好的模型" class="headerlink" title="5.开发比基准更好的模型"></a>5.开发比基准更好的模型</h3><p>这一阶段的目的是选择合理的网络架构，打败纯随机的基准。</p>
<ul>
<li>最后一层的激活</li>
<li>损失函数</li>
<li>优化器</li>
</ul>
<p>直接优化衡量问题成功的指标不一定总是可行的，例如ROC AUC就不能被直接优化，常用交叉熵来做为替代指标。<br>难以将指标转化为损失函数，是因为损失函数需要：</p>
<ul>
<li>在小批量数据时即可计算(理想情况下，只有一条数据时也可以计算)</li>
<li>可微分</li>
</ul>
<h3 id="6-开发过拟合的模型"><a href="#6-开发过拟合的模型" class="headerlink" title="6.开发过拟合的模型"></a>6.开发过拟合的模型</h3><p>理想的模型是刚好在欠拟合和过拟合的分界线上，在容量不足和容量过大的分界线上。为了找到这条分界线，必须要穿过它。<br>开发过拟合的模型可以：</p>
<ul>
<li>添加更多的层</li>
<li>让每一层变得更大</li>
<li>训练更多的轮次</li>
</ul>
<p>要始终监控训练损失和验证损失，以及所关心的指标的训练值和验证值。</p>
<h3 id="7-模型正则化与调节参数"><a href="#7-模型正则化与调节参数" class="headerlink" title="7.模型正则化与调节参数"></a>7.模型正则化与调节参数</h3><p>不断地调节模型、训练、在验证数据上评估，直到模型到达最佳性能。</p>
<ul>
<li>添加Drop out</li>
<li>添加L1, L2正则化</li>
<li>尝试减少层数</li>
<li>尝试不同的超参数</li>
<li>反复做特征工程(可选)</li>
</ul>
<h3 id="8-测试"><a href="#8-测试" class="headerlink" title="8.测试"></a>8.测试</h3><p>开发出满意的模型配置后，在所有可用数据(训练数据+验证数据)上训练最终的生产模型，然后在测试集上评估一次。<br>如果测试集的性能比验证集上差很多，意味着验证流程不可靠，可能出现了验证集上的过拟合。<br>此时需要评估验证流程，例如考虑选择更加可靠的评估方法。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/12/08/RL-Gym/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/08/RL-Gym/" itemprop="url">Gym</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-08T15:23:23+08:00">
                2018-12-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  548
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>OpenAI Gym由两部分组成：</p>
<ul>
<li>Gym开源库：测试问题集合（environments)，有共享的接口，允许用户设计通用算法</li>
<li>OpenAI Gym服务： 一个站点和API，允许用户对他们训练的算法进行性能比较</li>
</ul>
<h4 id="一般用法"><a href="#一般用法" class="headerlink" title="一般用法"></a>一般用法</h4><p>Gym包括几个系列的仿真环境：</p>
<ol>
<li>Classic control, Toy text：小规模任务，用于入门</li>
<li>Algorithmic：让Agent学习编程中的某些经典算法问题</li>
<li>Atari：雅达利游戏</li>
<li>2D and 3D robots: 机器人控制等</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line">env.reset()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></span><br></pre></td></tr></table></figure>
<p>gym.make()通过id调用环境；<br>reset()初始化环境变量，重置结束标志；<br>render()将环境变量绘制到屏幕上，非必须，只是为了观察方便；<br>step()采取一步动作，返回动作后的状态、回报等。</p>
<h6 id="step-函数"><a href="#step-函数" class="headerlink" title="step()函数"></a>step()函数</h6><p>step函数有四个返回值：</p>
<ol>
<li>observation (object)：对环境的观测值的组合</li>
<li>reward (float)：上一步动作的回报</li>
<li>done (boolean)：episode是否已结束，可由reset重置</li>
<li>info (dict)：调试信息，但不能用于agent学习过程</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">action = env.action_space.sample()</span><br><span class="line">observation, reward, done, info = env.step(action)</span><br></pre></td></tr></table></figure>
<h4 id="自定义问题"><a href="#自定义问题" class="headerlink" title="自定义问题"></a>自定义问题</h4><p>除了内置的环境，也可以自定义问题环境。<br>需要实现step(), reset(), render()函数等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义状态空间S</span></span><br><span class="line">self.states=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义动作空间A</span></span><br><span class="line">self.actions=[<span class="string">'n'</span>,<span class="string">'e'</span>,<span class="string">'s'</span>,<span class="string">'w'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义回报函数R</span></span><br><span class="line">self.rewards = dict()</span><br><span class="line">self.rewards[<span class="string">'1_s'</span>] = <span class="number">1.0</span></span><br><span class="line">self.rewards[<span class="string">'3_n'</span>] = <span class="number">-1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义状态转移矩阵T</span></span><br><span class="line">self.t = dict()</span><br><span class="line">self.t[<span class="string">'2_w'</span>] = <span class="number">1</span></span><br><span class="line">self.t[<span class="string">'2_e'</span>] = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实现step()函数,注意输入输出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    <span class="comment">#系统当前状态</span></span><br><span class="line">    state = self.state</span><br><span class="line"></span><br><span class="line">    <span class="comment">#判断系统当前状态是否为终止状态</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">in</span> self.terminate_states:</span><br><span class="line">        <span class="keyword">return</span> state, <span class="number">0</span>, <span class="keyword">True</span>, &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将状态和动作组成字典的键值</span></span><br><span class="line">    key = <span class="string">"%d_%s"</span>%(state, action) </span><br><span class="line"></span><br><span class="line">    <span class="comment">#状态转移</span></span><br><span class="line">    <span class="keyword">if</span> key <span class="keyword">in</span> self.t:</span><br><span class="line">        next_state = self.t[key]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        next_state = state</span><br><span class="line">    self.state = next_state</span><br><span class="line"></span><br><span class="line">    <span class="comment">#判断是否终止</span></span><br><span class="line">    is_terminal = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">if</span> next_state <span class="keyword">in</span> self.terminate_states:</span><br><span class="line">        is_terminal = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#判断回报</span></span><br><span class="line">    <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.rewards:</span><br><span class="line">        r = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r = self.rewards[key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_state, r,is_terminal,&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#实现绘图函数render()</span></span><br><span class="line"><span class="comment">#略</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实现重置函数reset()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_reset</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.state = self.states[int(random.random() * len(self.states))]</span><br><span class="line">    <span class="keyword">return</span> self.state</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/11/21/RL-ImportanceSampling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/21/RL-ImportanceSampling/" itemprop="url">重要性采样</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-21T15:52:59+08:00">
                2018-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  545
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h4><h6 id="离散型"><a href="#离散型" class="headerlink" title="离散型"></a>离散型</h6><p>$$ E(x) = \sum_{k=1}^{\infty} x_k p_k $$</p>
<h6 id="连续型"><a href="#连续型" class="headerlink" title="连续型"></a>连续型</h6><p>$$ E(x) = \int_{-\infty}^{\infty} xf(x) dx $$</p>
<h4 id="蒙特卡洛积分"><a href="#蒙特卡洛积分" class="headerlink" title="蒙特卡洛积分"></a>蒙特卡洛积分</h4><p>假设有某个函数f(x)，无法直接求出积分<br>$$ \int_{a}^{b} f(x) dx $$</p>
<p>这个函数在[a,b]上的积分，实际上是函数曲线和x轴围成的区域的面积，记做A。<br>那么在y轴上，必然存在一个位置h，使得h、a、b围成的长方形的面积也等于A。而这个h，就是函数f(x)在作用域[a,b]上的平均值。<br>所以问题转化为如何在[a,b]上求出f(x)的平均值。<br><img src="\img\mc1.jpeg"></p>
<p>蒙特卡洛方法是，在[a,b]内取大量的随机值$x_i$，计算$f(x_i)$，然后将所有的$f(x_i)$取平均值，作为h的估计值。<br>在实际应用中，只能取到有限的值，因而误差是不可避免的。因而如何使用更少的样本数量，得到误差更小的h值，也是改进的方向之一。<br>用一个公式来概括蒙特卡洛积分：<br>$$ \int_{a}^{b} f(x)dx \approx \frac{b-a}{N} \sum_{i=1}^{N} f(x_i)$$</p>
<h4 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h4><p>蒙特卡洛积分是在[a,b]上均匀采样，但对于非均匀分布的函数f(x)来说，函数值大的地方对平均值的影响更大，因而我们希望能在函数值大的地方多采集一些数据。<br>例如下图f(x)是高斯分布，两边的值对积分的贡献少，黄色的采样点就可以少一些。<br><img src="/img/mc2.jpeg"></p>
<p>因而引入概率密度函数$p(x_i)$，作为采样点对最终积分的重要度权重。<br>p(x)有一个重要的特性：<br>$$ \int_{-\infty}^{\infty} p(x)dx = 1 $$<br>于是我们得到新的积分函数：<br>$$ \int_a^b f(x)dx \approx \frac{1}{N} \sum_{i=0}{N} \frac{f(x_i)}{p(x_i)}$$</p>
<p><img src="/img/ImportanceSampling.png"></p>
<p>原分布密度P(s)，采样器密度Q(x)，权重<br>$$ w_r = \frac{P^{*}(x^{(r)})}{Q^{*}(x^{(r)})} $$</p>
<p>Q(x)中x的值低于P(x)的地方，权重将大于1，得到更多的表示；<br>Q(x)中x的值高于P(x)的地方，权重将小于1，降低表示强度。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/11/17/RL-TDLambda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/17/RL-TDLambda/" itemprop="url">TD($\lambda$)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-17T09:26:28+08:00">
                2018-11-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  846
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="0-n-step-TD"><a href="#0-n-step-TD" class="headerlink" title="0.n-step TD"></a>0.n-step TD</h4><p>TD方法除了之前介绍的1-step TD之外，还有一系列其他方法：<br><img src="/img/nstepTD.png"></p>
<p>n-step 的定义：<br>$$ G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n}) $$</p>
<p>迭代公式变为：<br>$$ V_{t+n}(S_t) = V_{t+n-1}(S_t) + \alpha[G_{t:t+n} - V_{t+n-1}(S_t)] $$</p>
<p>TD(0)可以看做n=1，MC可以看做n=T-t，是n取值的两种极端情况，往往不如n取中间值的结果。<br><img src="/img/n-stepError.png"></p>
<h4 id="1-Forward-lambda-return"><a href="#1-Forward-lambda-return" class="headerlink" title="1.Forward: $\lambda$-return"></a>1.Forward: $\lambda$-return</h4><p>$G_t^{\lambda}$ 将所有n-step returns $G_t^{(n)}$加权，使用权重$(1-\lambda)\lambda^{n-1}$<br>信号强度以速率$\lambda$减弱，因子(1-$\lambda$)是为了使等比数列权重的和为1.<br><img src="/img/TDWeight.png"></p>
<p>$$ G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)} $$<br>或者写作<br>$$ G_t^{\lambda} = (1-\lambda)\sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1} G_t $$</p>
<p>迭代公式变为：<br>$$ V(S_t) \leftarrow V(S_t) + \alpha(G_t^{\lambda} - V(S_t)) $$</p>
<p>由于$G_t^{\lambda}$是对所有$G_t^{(n)}$的加权，包含将来时刻的回报$R_{t+n}$，因而要等到episode结束才能更新，和MC一样是off-line算法。<br>对当前时刻t的更新，用到了未来时刻{t+1,t+2,…,t+n}的结果，因而称作前向视角(Forward)。<br><img src="/img/ForwardView.png"></p>
<h4 id="2-Backward"><a href="#2-Backward" class="headerlink" title="2.Backward"></a>2.Backward</h4><p>Forward中，当前V(s)的更新用到了许多未来的 $\delta$; 而Backward中，当前的$\delta$作用于许多过去的V(s’)，相当于对过去的V(s’)提供了一个“未来切片”。这个“未来切片”的权重，使用<strong>资格迹</strong>(Eligibility Trace)来衡量。</p>
<p>假设当前时刻t，过往序列{$s_{t-3},s_{t-2},s_{t-1},s_t$}的权重是{$(\gamma\lambda)^3, (\gamma\lambda)^2, \gamma\lambda, 1$}。在某个episode里，序列采样到了{$s_1,s_3,s_2,s_1$}，那么{$s_1,s_2,s_3$}的权重便是{$(\gamma\lambda)^3+1, \gamma\lambda, (\gamma\lambda)^2$}，将权重的算法抽离出来：</p>
<p><img src="\img\EligibilityTrace.png"></p>
<p>资格迹是一种兼顾频率和时间的权重，对每个状态独立计算，状态每出现1次权重加1，时间每前进1格权重按一定比例衰减。用于TD($\lambda$)算法中：</p>
<ul>
<li>状态空间的每个s对应一个资格迹</li>
<li>每次更新时，更新所有的V(s)</li>
<li>更新量为资格迹乘以TD-error </li>
</ul>
<p>每次对所有V(s)更新，就能覆盖到所有过去曾出现的状态；更新量乘以资格迹，就能按时间距离衰减。用当前时刻的TD Error $\delta_t$对过去出现的状态更新，称作后向视角(Backward)。</p>
<p><img src="\img\BackwardView.png"></p>
<p>当$\lambda = 0$时，过去全部衰减，$E_t(s)=1(S_t=s)$，更新只考虑当前步，退化为TD(0);<br>当$\lambda = 1$时，只有衰减因子$\gamma$，更新量和every-visit MC相同。</p>
<p>TD($\lambda$)提供了可实用的Online算法，相比$\lambda$-return有3方面的改进：</p>
<ul>
<li>在episode的每一步都会更新，而不用等到结束</li>
<li>计算过程分布平均，而并非集中在结尾</li>
<li>适用于连续过程等没有结束的情况</li>
</ul>
<p>以Sarsa($\lambda$)为例，每个(s,a)对应一个资格迹E(s,a)，每个step中都要更新所有的Q(s,a)，更新量$\alpha\delta E(s,a)$。因为这是一种增量算法，数据按顺序依次进入，前后数据来自同一过程的关联步骤。<br>而在DQN中，训练数据从数据集里抽样，打破了前后关联性，就不需要资格迹。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mirokule.github.io/2018/11/16/RL-Temporal-Difference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Miles">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gate of Babylon">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/16/RL-Temporal-Difference/" itemprop="url">Temporal-Difference(0)时序差分方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-16T15:02:58+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  967
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>时序差分学习(Temporal-Difference Learning)是MC和DP的结合，不要求完整的episode(无需终止状态)。<br>像MC一样Sample，像DP一样bootstrapping. Bootstrapping自举，是指当前状态的值函数计算用到了后继状态的值函数。<br>本文先讨论one-step TD,即TD(0).</p>
<h3 id="1-TD-Prediction"><a href="#1-TD-Prediction" class="headerlink" title="1.TD Prediction"></a>1.TD Prediction</h3><h4 id="1-1-算法"><a href="#1-1-算法" class="headerlink" title="1.1 算法"></a>1.1 算法</h4><p>输入要评估的策略$\pi$<br>随机初始化V(s)<br>Repeat(对每个episode):<br>&emsp; 初始化S<br>&emsp; Repeat(对episode的每一步):<br>&emsp;&emsp; A ← 策略$\pi$为状态S指定的动作<br>&emsp;&emsp; 执行动作A，观察R,S’<br>&emsp;&emsp; V(s) ← V(s) + $\alpha$[R+$\gamma$V(S’)-V(S)]<br>&emsp;&emsp; S ← S’<br>&emsp; 直到S为终止状态</p>
<h4 id="1-2-比较"><a href="#1-2-比较" class="headerlink" title="1.2 比较"></a>1.2 比较</h4><p>MC：根据每次的$G_t$来更新$V(S_t)$<br>$$ V(S_t) \leftarrow V(S_t) + \alpha(G_t - V(S_t))$$</p>
<p>TD(0): 用估计值$R_{t+1} + \gamma V(S_{t+1})$代替$G_t$<br>$$ V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) $$</p>
<p>TD和MC同样都是通过采样，多次实验(多个episode)来学习。<br>不同的是，TD利用了马尔科夫性质，总是收敛于最可能的马尔科夫解，因而在马尔科夫过程中表现更好。<br>MC没有利用马尔科夫性质，在非马尔科夫过程中表现更好。</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">Bootstrapping</th>
<th style="text-align:center">No Bootstrapping</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Sample</td>
<td style="text-align:center">Temporal Difference</td>
<td style="text-align:center">Monte Carlo</td>
</tr>
<tr>
<td style="text-align:left">No Sample</td>
<td style="text-align:center">Dynamic Programming</td>
<td style="text-align:center">Exhausitive Search</td>
</tr>
</tbody>
</table>
<h4 id="1-3-误差分析"><a href="#1-3-误差分析" class="headerlink" title="1.3 误差分析"></a>1.3 误差分析</h4><h6 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h6><p>Return $G_t = R_{t+1}+\gamma R_{t+2}+\cdots +\gamma^{T-1}R_T$ 无偏，因为期望值就是值函数的定义<br>True TD target $R_{t+1} + \gamma v_{\pi}(S_{t+1})$ 无偏，因为$v_{\pi}(S_{t+1})$是真实值，可以推导得到<br>TD target $R_{t+1} + \gamma V(S_{t+1})$ 有偏，因为$V(S_{t+1})$也是估计值</p>
<h6 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h6><p>Return $G_t$要等到最终状态出现，经过很多步，因而随机性大，方差很大。<br>TD target的方差比Return小得多，因为只有1步估计，引入的噪声少。</p>
<p>所以MC高方差，无偏差；TD低方差，有偏差。</p>
<h3 id="2-TD-Control"><a href="#2-TD-Control" class="headerlink" title="2.TD Control"></a>2.TD Control</h3><h4 id="2-1-On-Policy-Sarsa"><a href="#2-1-On-Policy-Sarsa" class="headerlink" title="2.1 On-Policy: Sarsa"></a>2.1 On-Policy: Sarsa</h4><p>遵循GPI框架，仍然分为两步：</p>
<ul>
<li>策略评估：使用动作值函数Q(s,a),因从(S,A)到(S’,A’)而得名</li>
<li>策略改进：仍然是$\epsilon -greedy$</li>
</ul>
<p>$$ Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma\,Q(S_{t+1},a) - Q(S_t, A_t)] $$</p>
<h6 id="Q-s-a-评估"><a href="#Q-s-a-评估" class="headerlink" title="Q(s,a)评估"></a>Q(s,a)评估</h6><p>随机初始化Q(s,a)<br>Repeat(对每个episode):<br>&emsp; 初始化S<br>&emsp; 从当前策略($\epsilon -greedy$)中选择S对应的动作A<br>&emsp; Repeat(对episode的每一步):<br>&emsp;&emsp; 执行动作A，观察R,S’<br>&emsp;&emsp; 从<strong>当前策略</strong>中选择S’对应的动作A’<br>&emsp;&emsp; Q(S,A) ← Q(S,A) + $\alpha$[R+$\gamma$Q(S’,A’)-Q(S,A)]<br>&emsp;&emsp; S ← S’; A ← A’;<br>&emsp; 直到S为终止状态</p>
<h6 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    action = epsilon_greedy_policy(state, epsilon)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        state_next, reward, done, _ = env.step(action)</span><br><span class="line">        action_next = epsilon_greedy_policy(state_next, epsilon)</span><br><span class="line">        </span><br><span class="line">        Q[state][action] += alpha*(reward + gamma*Q[state_next][action_next] - Q[state][action])</span><br><span class="line">        </span><br><span class="line">        state = state_next</span><br><span class="line">        action = action_next</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="2-2-Off-Policy-Q-learning"><a href="#2-2-Off-Policy-Q-learning" class="headerlink" title="2.2 Off-Policy: Q-learning"></a>2.2 Off-Policy: Q-learning</h4><ul>
<li>策略评估：行动策略采用$\epsilon-greedy$</li>
<li>策略改进：目标策略采用贪心策略，不使用$\epsilon-greedy$(和SARSA不同)，不需要Importance Sampling(和MC不同)</li>
</ul>
<p>类比DP中的Value Iteration，在更新时无视原策略，直接选用最大值，相当于默认使用贪心策略：<br>$$ Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma\,\underset{a}{max}Q(S_{t+1},a) - Q(S_t, A_t)] $$</p>
<p>注意$\underset{a}{max}Q(S_{t+1}, a)$是下一个状态的Q_table里Q值最大的那个动作。</p>
<h6 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        action = epsilon_greedy_policy(state, epsilon)</span><br><span class="line">        state_next, reward, done, _ = env.step(action)</span><br><span class="line">        </span><br><span class="line">        q_max = max([ Q[state_next][a] <span class="keyword">for</span> a <span class="keyword">in</span> range(env.action_space.n) ])</span><br><span class="line">        Q[state][action] += alpha * (reward + gamma * q_max - Q[state][action])</span><br><span class="line">        </span><br><span class="line">        state = state_next</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h4 id="2-3-比较"><a href="#2-3-比较" class="headerlink" title="2.3 比较"></a>2.3 比较</h4><p>SARSA和Q-Learning的区别在于更新$Q(S_t, A_t)$时对$Q(S_{t+1},a)$的选择：</p>
<ul>
<li>SARSA：根据$\epsilon-greedy$策略选出的动作$Q(S_{t+1},a)$，会受到其他Q值较小的动作a影响</li>
<li>Q-Learning：Q值最大的动作$\underset{a}{max}Q(S_{t+1},a)$，不会受到其他Q值较小的动作a影响</li>
</ul>
<p>两种方法在采取行动时的策略相同，都是$\epsilon-greedy$.<br><img src="/img/SARSA-Q.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Miles" />
            
              <p class="site-author-name" itemprop="name">Miles</p>
              <p class="site-description motion-element" itemprop="description">万人迷</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">336</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/mirokule" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:miles.miro@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.kaggle.com/" title="Kaggle" target="_blank">Kaggle</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://unity3d.com/" title="Unity" target="_blank">Unity</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.apple.com/swift/" title="Swift" target="_blank">Swift</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">M.M.Tech</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">125.5k</span>
  
</div>










        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
